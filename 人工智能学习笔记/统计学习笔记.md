# 统计学习方法笔记

## 引言

参考：

2012-李航-统计学习方法

## 统计学习方法概论

### 统计学习

1. 统计学习的特点：

   （1） 以计算机及网络为平台，建立在计算机及网络之上

   （2） 以数据为研究对象

   （3） 对数据进行预测与分析

   （4） 以方法为中心

2. 统计学习的对象：数据----分为连续变量和离散变量

3. 统计学习的目的考虑学习什么样的模型和如何学习模型，以使模型能对数据进行准确的预测与分析，同时也要考虑尽可能地提高学习效率

4. 统计学习的方法：

   分为：监督学习，非监督学习，半监督学习，强化学习

   实现统计学习方法的步骤：

   （1） 得到一个有限的训练集合

   （2） 确定包含所有可能的模型的假设空间，即学习模型的集合

   （3） 确定模型选择的准则，即学习策略

   （4）实现求解最优模型的算法，即学习的算法

   （5）通过学习方法选择最优模型

   （6） 利用学习的最优模型对新数据进行预测或分析

5. 统计学习的研究

   包括：统计学习方法，统计学习理论，统计学习应用

6. 统计学习的重要性

### 监督学习

监督学习的任务是：学习一个模型，使模型能够对任意给定的输入，对其相应的输出做出一个好的预测.

#### 基本概念

**输入空间**：输入所有可能取值的集合

**输出空间**：输出所有可能取值的集合

**实例**：某一个具体的输入

**特征空间**：所有特征向量存在的空间

输入与输出的随机变量X和Y遵循联合概率分布P(X,Y)是**监督学习的基本假设**

**假设空间**：输入空间到输出空间的映射的集合，代表着学习范围

### 统计学习三要素

统计学习方法都是由模型，策略和算法构成的，即：

​												`方法=模型+策略+算法`

#### 模型

在监督学习中，模型就是所要学习的条件概率分布或决策函数

模型的假设空间包含所有可能的条件概率分布或决策函数

假设空间可以定义为决策函数的集合
$$
F=\{f|Y=f(X)\}
$$
X,Y是定义在输入空间和输出空间上的变量，F通常是由一个参数向量决定的函数族
$$
F=\{f|Y={f}_θ(x),θ\in{R}^n\}
$$
参数向量θ取值于n维欧氏空间，称为参数空间

假设空间还可以定义为条件概率的集合
$$
F=\{P|P(Y|X)\}
\\
F=\{P|P_θ(Y|X),θ\in R^n\}
$$
**由决策函数表示的模型为非概率模型，由条件概率表的模型为概率模型**

#### 策略

**损失函数**度量模型一次预测的好坏

**风险函数**度量平均意义下模型预测的好坏

常用的几种损失函数：

+ 0-1损失函数
+ 平方损失函数
+ 绝对损失函数
+ 对数损失函数

损失函数的期望也就是理论上模型关于预测的平均意义下的损失，也称为**风险函数**

**经验风险**：模型关于训练样本的平均损失

对经验风险进行矫正的策略：

+ 经验风险最小化：经验风险最小的模型是最优的模型，例如：极大似然估计
+ 结构风险最小化：等价于正则化，在经验风险最小化的基础上，增加模型复杂度的正则化项或罚项，例如：贝叶斯估计

#### 算法

**算法**：是指学习模型的具体计算方法

### 模型评估与模型选择

**损失函数并不一定是评估时使用的损失函数**

**训练误差**：模型关于训练数据集的平均损失

**测试误差**：模型关于测试数据集的平均损失

**过拟合**：学习时选择的模型所包含的参数过多，以至于出现这一模型对已知数据预测得很好，但对位置数据预测很差

### 正则化与交叉验证

**正则化项**：一般是模型复杂度得单调递增函数，模型越复杂，正则化值越大

**奥卡姆剃刀原理(简约法则)**：切勿浪费多余功夫去做本可以较少功夫完成之事

在该处表示为：在所有可能选择的模型中，能够很好的解释已知数据并且十分简单才是最好的模型，也就是应该选择的模型



在样本数据充足的情况下，随机将数据集切分为三部分，分别为：训练集，验证集，测试集

训练集用于训练模型，验证集用于模型选择，测试集用于最终对学习方法的评估



**交叉验证**：重复使用数据，把给定的数据进行切分，将切分的数据集组合为训练集与测试集，在此基础上反复训练，测试以及模型选择

+ 简单交叉验证：将数据分为两个部分，训练集和测试集，然后进行训练和测试
+ S折交叉验证：随机将已给数据切分为S个互不相交的大小相同的子集；然后利用S-1个子集的数据训练模型，利用余下的子集测试模型；将这一过程对可能的S种选择重复进行；最后选出S次评测中评价测试误差最小的模型
+ 留一交叉验证：在数据缺乏时使用，这时`S=N`，N为给定数据集的容量

### 泛化能力

**泛化能力**：由该方法学习到的模型对未知数据的预测能力

**泛化误差**：对未知数据预测的误差
$$
R_{exp}(f)=E_p[L(Y,f(X))]=\int_{X*Y}(L(y,f(x)))P(x,y)dxdy
$$
泛化误差上界通常具有的性质：

+ 是样本容量的函数，当样本容量增加时，泛化上界趋于0
+ 是假设空间的函数，假设空间容量越大，模型越难学习，泛化误差上界越大

**定理**：

对二类分类问题，当假设空间时有限个函数的集合`F={f1,f2,...,fd}`时，对任意一个函数f∈F，至少以概率`1-θ`,则有以下不等式成立
$$
期望风险: R(f)=E[L(Y,f(X))]\\
经验风险: \hat{R}(f)= \frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))\\
R(f)<=R(f)+ɛ(d,N,θ)\\
其中，ɛ(d,N,θ)=\sqrt[2]{\frac{1}{2N}(logd+log\frac{1}{θ})}
$$
不等式左侧`R(f)`是泛化误差，右端为泛化误差上界

### 生成模型与判别模型

监督学习方法可分为**生成方法**和**判别方法**，对应学习到的模型称为**生成模型**和**判别模型**

##### 生成方法

由数据学习联合概率分布P(X,Y)，然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型
$$
P(Y|X)=\frac{P(X,Y)}{P(X)}
$$
典型的生成模型有：**朴素贝叶斯法**和**隐马尔可夫模型**

特点：

+ 可以还原出联合概率分布
+ 学习收敛速度快，容量越大，收敛速度越快
+ 存在隐变量，依旧适用

##### 判别方法

由数据学习直接学习决策函数f(X)或条件概率分布P(Y|X)作为预测的模型

典型的判别模型包括：k近邻法，感知机，决策树，逻辑斯谛回归模型，最大熵模型，支持向量机，提升方法和条件随机场

特点：

+ 直接面对预测
+ 学习的准确率较高
+ 可以简化学习问题

### 分类模型

**分类器**：监督学习从数据中学习一个分类模型或分类决策函数

**分类**：分类器对新的输入进行输出的预测



分类问题包括学习和分类两个部分：

+ 学习：根据一致的训练数据集利用有效的学习方法学习一个分类器
+ 分类：利用学习的分类器对新的输入实例进行分类



**分类准确率**：对于给定的测试数据集，分类器正确分类的样本数和总样本数之比

二分类问题常用的评价指标是**精确率**和**召回率**

### 标注问题

标注问题的目标在于学习一个模型，使它能够对观测序列给出标记序列作为预测

标注问题分为学习和标注两部分

评价标注模型的指标与评价分类模型指标相同

标注常用的统计学习方法有：隐马尔可夫模型，条件随机场

### 回归问题

回归模型表示从输入变量到输出变量之间映射的函数

**回归问题的学习等价于函数拟合**

按照输入变量的个数，分为：一元回归和多元回归

按照输入变量与输出变量之间关系的模型，分为：线性回归和非线性回归

常用的损失函数：**平方损失函数**

最经典的方法：**最小二乘法**

## 感知机

感知机是**二类分类**的**线性分类模型**，输入为实例的特征向量，输出为实例的类别，取+1和-1二值

感知机属于**判别模型**，是神经网络与支持向量机的基础

感知机学习算法分为：原始形式和对偶形式

### 感知机模型

**感知机定义**：

​	假设输入空间是`X`，输出空间是`Y={+1,-1}`输入`x`表示实例的特征向量，对应于输入空间的点；输出`y`表示实例的类别。由输入空间到输出空间的如下函数：
$$
f(x)=sign(w*x+b)\\
$$
称为感知机

w和b为感知机模型参数，w为权值向量，b为偏置

sign为符号函数
$$
sign(x)=\left\{
\begin{aligned}
+1,\quad x>=0\\
-1,\qquad x<0\\
\end{aligned}
\right.
$$
感知机的模型假设空间是定义在特征空间中的所有线性分类模型或线性分类器，即函数集合

### 感知机学习策略

**数据集的线性可分性**：

给定一个数据集，如果存在某个超平面S能够将数据集的正实例点和负实例点完全正确的划分到超平面两侧，则称数据集T为线性可分数据集，否则，称为线性不可分





### 感知机学习算法

## k近邻法

### k近邻算法

### k近邻模型

### k近邻法的实现：`kd`树

## 朴素贝叶斯法

### 朴素贝叶斯法的学习与分类

### 朴素贝叶斯法的参数估计

## 决策树

### 决策树模型与学习

### 特征选择

### 决策树的生成

### 决策树的剪枝

### CART算法

## 逻辑斯谛回归与最大熵模型

### 逻辑斯谛回归模型

### 最大熵模型

### 模型学习的最优化算法

## 支持向量机

### 线性可分支持向量机与硬间隔最大化

### 线性支持向量机与软间隔最大化

### 非线性支持向量机与核函数

### 序列最小最优化算法

## 提升方法

### 提升方法AdaBoost算法

### AdaBoost算法的训练误差分析

### AdaBoost算法的解释

### 提升树

## EM算法及其推广

### EM算法的引入

### EM算法的收敛性

### EM算法在高斯混合模型学习中的应用

### EM算法的推广

## 隐马尔可夫模型

### 隐马尔可夫模型的基本概念

### 概率计算算法

### 学习算法

### 预测算法

## 条件随机场

### 概率无向图模型

### 条件随机场的定义与形式

### 条件随机场的概率计算问题

### 条件随机场的学习算法

### 条件随机场的预测算法

## 统计学习方法总结