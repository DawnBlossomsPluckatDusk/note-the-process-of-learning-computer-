# 神经网络与深度学习

## 参考资料

[神经网络与深度学习--丘锡鹏](https://nndl.github.io/)

## 机器学习基础

### 第一章 绪论

#### 简谈深度学习(Deep Learning)

+ 定义：

  ​	从有限样例中通过算法总结出一般性的规律，并可以应用到新的未知数据上。

+ 实现：

  ​	通过一个或多个线性或非线性的组件构成一个完整的模型，输入的数据通过这些组件进行加工，最后得出结果。

+ 挑战：

  ​	贡献度分配问题(Credit Assignment Problem, CAP):

  ​	在深度学习中，每个组件对输出结果的贡献是未知的，相差一点的贡献可能会带来截然不同的结果。

#### 简谈人工神经网络(Artificial Neural Network ,ANN)

+ 定义：

  ​	受人脑神经系统的工作方式启发而构造的数学模型。

+ 功能：

  ​	可以较好的实现深度学习，是一个不错的`模型`

+ 实现：

  ​	由人工神经元以及神经元之间的连接构成，其中有两类特殊的神经元：

  + 接受外部的信息---输入
  + 输出信息---输出

  通过机器学习的方式，将数据转化为连接神经元之间的参数

#### 人工智能的定义及主要理论

+ 定义：

  ​	人工智能就是要让机器的行为看起来就像是人所表现出的智能行为一样

+ 主要领域：

  + 感知：模拟人的感知能力：语音信息处理和计算机视觉...
  + 学习：模拟人的学习能力：监督学习，无监督学习和强化学习...
  + 认知：模拟人的认知能力：知识表示，自然语言理解，推理，规划，决策...

#### 人工智能的发展历史

1.  推理期：

   基于逻辑或事实归纳，编写对应的程序，从而完成特定的任务---对人工智能的不完全认识导致过于乐观，低估了人工智能的难度

2. 知识期：

   专家系统为这一时期的特色，通过“知识库+推理机”的结构实现解决特定的专业领域的问题

   专家系统的三要素：

   + 领域专家级知识
   + 模拟专家思维
   + 达到专家级的水平

3. 学习期：

   机器学习兴起，让计算机从数据中学习，获得解决问题的方法。机器学习主要就是：设计和分析一些学习算法，让计算机可以从数据中学习经验，从而解决未知的问题

   #### 人工智能的流派

   1. 符号主义：

      + 观点：只要在符号计算上实现了相应的功能，那么现实世界就实现了对应的功能。----只要在机器上是正确的，现实世界也是正确的
      + 成就：专家系统和知识工程
      + 挑战：
        - 知识组合爆炸
        - 命题组合悖论：两个真的命题合起来不一定是真命题
        - 经典概念在实际生活中难以取得

   2. 连接主义：

      + 观点：大脑是一切智能的基础，要关注于大脑神经元及其连接机制，在机器上实现对人脑的模拟

      + 成就: 成为广泛运用的AI实现路线

      + 挑战：对人脑结构的不确定，真正的神经网络和深度学习离现在还有不远的距离

        

   3. 行为主义(书中没有介绍)：

      + 观点：智能只取决于感知和行动，不需要知识，推理和表示，只要将智能行为表现出来就可以了
      + 成就：机器人控制
      + 挑战：`莫拉维克悖论`：对计算机来说，最困难的往往是人类技能中那些无意识的技能



#### 简谈机器学习(Machine Learning,ML )

1. 定义：指从有限的，可观测的数据中学习出具有一般性的规律，并利用这些规律对未知数据进行预测的方法

2. 传统机器学习模型的一般步骤：

   +  数据预处理：对数据的原始形式进行初步的数据清理和加工，构建为训练机器学习模型的数据集

   + 特征提取：从数据中提取一些对特定机器学习任务有用的高质量特征---初加工

   + 特征转换：对特征进行进一步加工----细加工

   + 预测：学习一个函数并进行预测---核心

     ![传统机器学习的数据处理流程](C:\Users\19278\AppData\Roaming\Typora\typora-user-images\image-20210902182003267.png)

#### 表示学习(Representation Learning)

+ 定义：指能够自动地学习出有效的特征，并提高最终机器学习模型地性能
+ 挑战：语义鸿沟(Semantic Gap)问题：指输入数据的底层特征和高层语义信息之间地不一致性和差异性。

+ 问题：1. 什么是好的表示；2. 如何学习到好的表示

常用的两种表示方法：

1. 局部表示----通常表示为one-hot向量的形式
2. 分布式表示

局部表示的优点：

+ 离散的表示方式具有很好的解释性，有利于人工归纳和总结特征，并通过特征组合进行高效的特征工程
+ 通过多种特征组合得到的表示向量通常是稀疏的二值向量，当用于线性模型时计算效率非常高

局部表示的缺点：

+ one-hot向量的维数很高，且不能扩展
+ 不同维度之间的相关性都为0



**分布式表示：**

通过几个较低维度相互组合，可以达到在局部表示中的不同维度的效果。(**嵌入**：指将一个度量空间中的一些对象映射到另一个低维的度量空间中，并尽可能保持不同对象之间的拓扑关系)



#### 深度学习

主要目的：从数据中自动学习到有效的特征表示

深度学习的处理流程：

![image-20211021103533780](C:\Users\19278\AppData\Roaming\Typora\typora-user-images\image-20211021103533780.png)

需要解决的关键问题：贡献度分配问题(Credit Assignment Problem CAP) ，即一个系统中不同的组件或其参数对最终系统输出结果的贡献或影响。

目前深度学习采用的模型主要是**神经网络模型**，主要原因是：神经网络模型可以使用误差反向传播算法，从而可以较好地解决贡献度分配问题



传统机器学习方法：将一个任务切分为多个子模块，每个子模块分开学习。主要的问题：

+ 每个模块都需要单独优化，并且其优化目标和任务总体目标并不能保证一致
+ 错误传播，即前一步的错误会对后续的模型造成很大的影响

端到端学习(端到端训练)：指在学习过程中不进行分模块或分阶段训练，直接优化任务的总体目标。



#### 神经网络

神经网络是指：由很多人工神经元构成的网络结构模型，这些人工神经元之间的连接强度是可学习的参数



赫布理论：当神经元A的一个轴突和神经元B很近，并对它产生影响，并且持续地，重复地参与了对神经元B的兴奋，那么在这两个神经元或其中之一会发生某种生长过程或新陈代谢变化，以致神经元A作为能使神经元B兴奋的细胞之一，他的效能增强了

人工神经网络通过模拟人脑神经网络而设计的一种**计算模型**

网络容量(Network Capacity) ：一个人工神经网络塑造复杂函数的能力

神经网络的发展：

1. 1943~1969 模型提出----第一个高潮期
2. 1969~1983 冰河期-----第一个低谷期
3. 1983~1995 反向传播算法复兴---第二个高潮期
4. 1995~2006 流行度降低----第二次低谷期
5. 2006~至今 深度学习----第三次高潮期



### 第二章 机器学习概述

机器学习通常是指：一类问题以及解决这类问题的办法，即如何从观测样本中寻找规律，并利用学习到的模型对未知或无法观测的数据进行预测

#### 基本概念

+ 特征(feature)
+ 标签(label)
+ 样本(sample)或实例(instance)：一个标记好特征以及标签的数据
+ 数据集：训练集+测试集
+ 特征向量(feature vector) ：由所有的特征组成的向量

机器学习系统：

![image-20211028140607114](C:\Users\19278\AppData\Roaming\Typora\typora-user-images\image-20211028140607114.png)

#### 机器学习的三个基本要素

##### 1. 模型：

不同机器学习任务的主要区别在于输出空间不同。

主要分为**线性模型**和**非线性模型**

在线性模型中，存在对应特征向量的权重和偏置

在非线性模型中，由多个非线性基函数组成

##### 2.学习准则：

一个好的模型应该能够为每个样本打上正确的标签

模型的好坏可以通过期望风险来衡量。

损失函数用来量化两个变量之间的差异。

常用的损失函数：

+ 0-1损失函数----比较客观，但很难优化
+ 平方损失函数---不适用于分类问题
+ 交叉熵损失函数(负对数似然函数)----用于分类问题
+ Hinge损失函数-----二分类问题



经验风险最小化准则：在给定训练集上无法得到期望风险，只能得到经验风险，即平均损失

**过拟合**：(定义)给定一个假设空间ℱ，一个假设𝑓 属于ℱ，如果存在其他的假设𝑓′ 也属于ℱ, 使得在训练集上𝑓 的损失比𝑓′ 的损失小，但在整个样本空间上𝑓′ 的损失比𝑓 的损失小，那么就说假设𝑓 过度拟合训练数据

经验风险最小化原则很容易导致模型在训练集上错误率很低，但是在未知数据上错误率很高，也称为过拟合

产生过拟合的原因：

+ 训练数据少
+ 噪声以及模型能力强

解决的办法：在经验风险最小化的基础上引入正则化来限制模型能力-----结构风险最小化



欠拟合：模型不能很好地拟合训练数据，在训练集上的错误率较高。

产生原因：模型能力不足

![image-20211028144534240](C:\Users\19278\AppData\Roaming\Typora\typora-user-images\image-20211028144534240.png)



##### 3. 优化算法：



#### 机器学习的简单示例

#### 偏差-方差分解

#### 机器学习算法的类型

#### 数据的特征表示

#### 评价指标

#### 理论和定理

### 第三章 线性模型

#### 线性判断函数和决策边界

#### `Logistic` 回归

#### `Softmax`回归

#### 感知器

#### 支持向量机

#### 损失函数对比

## 基础模型

### 第四章 前馈神经网络

#### 神经元

#### 网络结构

#### 前馈神经网络

#### 反向传播算法

#### 自动梯度计算

#### 优化问题

### 第五章 卷积神经网络

#### 卷积

#### 卷积神经网络

#### 参数学习

#### 几种典型的卷积神经网络

#### 其他卷积方式

### 第六章 循环神经网络

#### 给网络增加记忆能力

#### 简单循环网络

#### 应用到机器学习

#### 参数学习

#### 长程依赖问题

#### 基于门控的循环神经网络

#### 深层循环神经网络

#### 扩展到图结构

### 第七章 网络优化与正则优化

#### 网络优化

#### 优化算法

#### 参数初始化

#### 数据预处理

#### 逐层归一化

#### 超参数优化

#### 网络正则化

### 第八章 注意力机制与外部记忆

#### 认知神经学中的注意力

#### 注意力机制

#### 自注意力模型

#### 人脑中的记忆

#### 记忆增强神经网络

#### 基于神经动力学的联想记忆

### 无监督学习

#### 无监督特征学习

#### 概率密度估计

### 第十章 模型独立学习方式

#### 集成学习

#### 自训练和协同训练

#### 多任务学习

#### 迁移学习

#### 终身学习

#### 元学习

## 第三部分 进阶模型

### 第十一章 概率图模型

#### 模型表示

#### 学习

#### 推断

#### 变分推断

#### 基于采样法的近似推断

### 第十二章 深度信念网络

#### 玻尔兹曼机

#### 受限玻尔兹曼机

#### 深度信念网络



### 第十三章 深度生成网络

#### 概率生成模型

#### 变分自编码器

#### 生成对抗网络



### 第十四章 深度强化学习

#### 强化学习问题

#### 基于值函数的学习方法

#### 基于策略函数的学习方法

#### 演员-评论员算法

### 第十五章 序列生成算法

#### 序列概率模型

#### N元统计模型

#### 深度序列模型

#### 评价方法

#### 序列生成模型中的学习问题

#### 序列到序列模型





